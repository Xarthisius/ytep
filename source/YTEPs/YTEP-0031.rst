.. _ytep-0031:

YTEP-0031: Removing the global octree mesh for particle data
============================================================

Abstract
--------

| Created: February 9 2017
| Author: Nathan Goldbaum, Meagan Lang, Matthew Turk

The global particle octree index used by yt presents a barrier for improving the
performance and scalability of visualizing and analyzing particle datasets. This
YTEP proposes removing the global octree index, replacing it with a combination
of a new IO system and changes to the high-level yt API to focus on returning
particle-centric data. The particle I/O refactor makes use of an indexing scheme
based on compressed Morton bitmaps which dramatically improves memory usage and
scaling for large particle datasets without the need for a global octree index.

Rather than constructing a global index to maintain backward compatibility at
the cost of scaling and performance, we instead propose a reworking of the yt
user interface for particle and SPH data to be more "particle-centric". This
means that data object selections for fields that are now defined on the global
octree mesh will instead return field data at particle locations. For SPH data,
visualizations of slices and projections are done in the image plane, making use
of the "scatter" approach by smoothing SPH data directly onto images, employing
either a volumetric or projected SPH smoothing kernel. Fully local derived
fields are calculated using yt's existing field definitions but passing in data
defined at particle locations. Fields that need spatial derivatives are
implemented using the SPH formalism and are also evaluated at the particle
locations.

Altogether these changes allow for improved performance and scaling, and allow
users to access, analyze, and visualize particle field data for SPH simulations
in a more straightforward fashion. While we do not propose substantial API
changes for mesh or octree codes, these changes to yt's field system for
particle data imply substantial changes to the *meaning* of yt's data selection
system for particle data. We discuss the implications of these backward
incompatible changes and how we intend to document and manage them in a way that
is minimally disruptive to users.

Status
------

In Progress. The implementation is mostly finished, although there are a few
features that still need to be implemented.

Project Management Links
------------------------

The code can be found in pull request 2382:

https://bitbucket.org/yt_analysis/yt/pull-requests/2382

The C++ compressed bitmap implementation we intend to vendor into yt:

https://github.com/lemire/EWAHBoolArray

Detailed Description
--------------------

Background
**********

Currently most user-facing operations on SPH data are produced by interpolating
SPH data onto a volume-filling octree mesh. When support for SPH data was added
to yt in the run-up to the yt-3.0 release, this approach allowed yt to support
SPH data in a way that could reuse the existing infrastructure in yt for octree
data and preserve core assumptions in yt that ``gas`` fields must correspond to
a volume-filling AMR structure. While this did make initial support for SPH data
much easier, it also had some downsides. In particular, the memory and CPU
overhead of smoothing SPH data onto the octree can be prohibitive on particle
datasets produced by large simulations. Visualizations of slices and projections
produced by yt using the default ``n_ref`` and ``over_refine_factor`` are
somewhat blocky since by default we use a relatively coarse octree to preserve
memory. In addition, since we construct the global octree based on the positions
of all particles, visualizations that include only one particle type sometimes
include "holes" in regions under-sampled by that particle type.

These cosmetic and semantic issues are jarring to users of SPH codes, who tend
to think of data defined at the particle locations rather than sampled onto an
adaptive mesh. Making our high-level API focus more on particle-centric data
will help to ease the cognitive dissonance felt by users of SPH codes when they
work with yt.

Over the past two years, Meagan Lang and Matt Turk implemented a new approach
for handling I/O of particle data, based on storing compressed bitmaps
containing Morton indices instead of an in-memory octree. This new capability
means that the global octree index is now no longer necessary to enable I/O
chunking and spatial indexing of particle data in yt.

In this document we describe the approach we take for replacing the global
octree index with Morton bitmasks. First, we describe the implementation of the
Morton bitmask index, changes to the low-level selector API needed to support
the Morton bitmask work, and the testing strategy used for the Morton bitmap
indexing scheme. Next we discuss high-level changes to how yt handles particle
data, discussion changes to the field system for SPH data, the implementation of
the SPH pixelizer system for visualizations, a discussion of deposit fields, and
a description of the strategy used to test the new approach for SPH data. We
close with a discussion of questions that still need to be tackled before this
work can be merged.


Low-Level Implementation
************************

Morton Bitmap Index
+++++++++++++++++++

The generated index serves to map how the domain is populated by particles in
datasets split across multiple files. This way, spatial queries can skip files
that do not contain particles in the selected part of the domain. The files are
mapped by storing two nested Morton indices for each particle in a
dataset. Rather than storing the indices in plaintext, we make use of an EWAH
compressed boolean array bitmap. Given a domain with known boundaries in each
dimension, a 3-dimension position can be described by a single integer `Morton
index <https://en.wikipedia.org/wiki/Z-order_curve>`_ by

1. Dividing the domain into ``2^index_order1`` cells in each dimension with
   widths ``ddx = domain_width_x/(2^index_order1)``.
#. Determining the 3 integers specifying the cell that contains the 3D
   position (e.g. ``x/ddx``).
#. Combine the 3 integers into a single integer by alternating bits from each
   dimension.

These indices can be stored as either integers or in boolean masks. In the case
of the mask, an array of zeroed bits is created with a length equal to the
maximum possible index for the chosen value of ``index_order1``. Then the bits
for the indices present are set to one. To save space, boolean masks in the form
of bitmaps can then be compressed further using the `Enhanced Word-Aligned
Hybrid (EWAH) bitmap compression algorithm
<https://doi.org/10.1145/1458432.1458434>`_. In practice, we make use of a
vendored version of `EWAHBoolArray <https://github.com/lemire/EWAHBoolArray>`_,
a C++ EWAH bitmask implementation available under the Apache v2 license.

One bitmap is created for each file. If an index is present in more than one
file’s bitmap, this represents a collision and decreases the likelihood that
the bitmap can be used to exclude files during spatial queries. This is unlikely
if the particles are well partitioned between the files according to a domain
decomposition scheme at the chosen order, but this is not generally true of
particle datasets produced by astrophysical simulations. In these cases, it
is better to create a more refined index.

Using a larger ``index_order1`` increases the refinement of the index, but also
increases both the memory required to store the indices and the time required
to query them for EWAH bitmaps. To combat this, we include a second refined
index within those cells that have indices in multiple files’ bitmaps for the
coarse index. For each particle with a coarse index that collides with another
file, a second refined Morton index is creating by following the same procedure
as for the coarse index, but exchanging the domain boundaries for the boundaries
of the coarse index cell. The refined index for each file is then stored in a
EWAH bitmap for each coarse cell with a collision.

The coarse and refined indices are generated in two separate I/O passes over the
entire dataset. To generate the coarse index, the coordinates of all particles,
as well as the softening lengths for SPH particles, are read in from each
file. For each particle, we then compute the Morton index corresponding to the
particles position within the domain. This index, ``mi`` is then used to set the
``mi``\ th element in a boolean mask for the file to 1. If the particle is an
SPH particle, neighboring indices with cells that overlap a sphere with a radius
equal to the particle’s softening length and centered on the particle are also
set to 1.

Once a coarse boolean mask is obtained for each file, the masks are stored in
a set of EWAH compressed bitmaps (``ewah_bool_array`` Cython extension
classes). Using logical boolean operations, we then identify those indices
that are set to 1 in more than one file’s mask (the collisions).

During a second I/O pass over the entire dataset, refined indices are created
for those particles with colliding coarse indices. Both the coarse and refined
indices are stored in an array for each file. One a file has been completely
read in, those indices are sorted and used to create a map from coarse indices
to EWAH compressed bitmaps. This is done because entries in EWAH compressed
bitmaps must be set in order.

The Morton bitmap index is created for each particle dataset upon its first
ingestion into yt and saved to a sidecar file. At all future ingestions of the
dataset into yt, the index will be loaded from the sidecar file. Indexes are
managed through the Cython extension class ``ParticleBitmap`` (defined in
``yt/geometry/particle_oct_container.pyx``), which is exposed to the user
visible yt API via the ``regions`` attribute of the ``ParticleIndex`` class
(e.g. ``ds.index.regions``). The ``ParticleBitmap`` class generates EWAH bitmaps
via the ``BoolArrayCollection`` Cython extension object (defined in
``yt/utilities/lib/ewah_bool_wrap.pyx``), which wraps the underlying
``EWAHBoolArray`` C++ library.

In the current implementation users can control the creation of the bitmask
index via the ``index_order`` and ``index_filename`` keyword arguments accepted
by ``SPHDataset`` instances. These keyword arguments replace the deprecated
``n_ref``, ``over_refine_factor`` and ``index_ptype`` keyword arguments. The
``index_order`` is a two-element tuple corresponding to the maximum Morton order
for the coarse and refined index. Using a tuple for the ``index_order`` instead
of two keyword arguments is not only more terse, but it will allow us to produce
bitmask indexes in the future with multiple refined indices while maintaining
the same public API. Currently the default ``index_order`` is ``(7, 5)``. If a
user specifies ``index_order`` as an integer, the integer is taken as the order
of the coarse index and the order of the refined index is set to ``1``,
producing a trivial refined index. For example::

  import yt
  ds = yt.load('snapshot_033/snap_033.0.hdf5',
               index_order=(5, 3), index_filename='my_index')
  ds.index

Running this script will produce the following output::

  yt : [INFO     ] 2017-02-14 11:50:20,815 Allocating for 4.194e+06 particles
  Initializing coarse index at order 5: 100%|██████| 12/12 [00:00<00:00, 14.60it/s]
  Initializing refined index at order 3: 100%|█████| 12/12 [00:01<00:00,  8.80it/s]

And produce a file named ``my_index`` in the same folder as
``snapshot_033/snap_033.0.hdf5``. The second and all later times the script is
run we only need to load the index from disk, so it produces the following
output::

  yt : [INFO     ] 2017-02-14 11:56:07,977 Allocating for 4.194e+06 particles
  Loading particle index: 100%|███████████████████| 12/12 [00:00<00:00, 636.33it/s]

Note that there 12 iterations for each loop. Each of these iterations correspond
to a single IO chunk. If a file has fewer than 262144 particles, the entire file
is used as an IO chunk. If a file has more than 262144 particles, the file is
logically split into several subfiles, each containing up to 262144
particles. Currently the chunk size of 262144 particles is hard-coded for all
SPH frontends.

Data Selection and Changes to the Selector API
++++++++++++++++++++++++++++++++++++++++++++++

The Morton bitmaps needed for individual data objects are constructed using the
existing low-level Cython selection API. To determine whether a given Morton
index is "contained" in the geometric primitive defined by the selector we make
use of the ``select_bbox`` selection API call, since each index corresponds to a
single cell in an octree. If the selector fully encloses the bounding box for
the cell defined by a given Morton index, the existing ``select_bbox`` function
is sufficient. However, given that the goal of the Morton bitmap index is to
reduce the number of files we need to read from for a given selection operation,
more care must be taken near the "edges" of a selector. For this reason, we have
added a new function to the selector API, ``select_bbox_edge``. This function is
identical to ``select_bbox`` in the case when a bounding box is fully contained
inside of the geometric primitive associated with a selector, simply returning 1
in these cases. However, if the bounding box is only partially contained in the
geometric primitive, ``select_bbox_edge`` returns 2, indicating partial
overlap. This is used in the bitmap index code to indicate that the coarse
Morton index does not have sufficient resolution in this region, triggering the
generation of refined Morton indices in this region. These smaller bounding
boxes will have a higher probability of being either fully contained or fully
excluded from a data object, decreasing the probability of a file collision. The
``select_bbox_edge`` function has been implemented for all selectors and if this
YTEP is accepted will be a required part of the API for new selectors in the
future.

In addition to the above change, a more minor change was necessary to the
portion of the selector API used to count and select particles contained in a
given selector. Currently, all particles are assumed to be pointlike, which will
lead to incorrect selections for particles that actually have finite volumes
like SPH particles. To account for this, the signature of the ``count_points``
and ``select_points`` functions were changed so that instead of accepting only
single scalar radius for all particles, they can accept an array of possibly
variable radii as well. If non-zero radii are passed in, particle selection
operates via the ``select_sphere`` method instead of the ``select_point`` method
that is currently used. Since some selectors did not yet have implementations
of ``select_sphere``, we have added new implementations where necessary.

Testing
+++++++

Removing the Global Octree Mesh
*******************************

Currently, all i/o operations are mediated via the global octree
index. Particles are read in from the output file as needed based on their
position in the octree.  With the arrival of the compressed bitmap index scheme
described above, we no longer need to use the global octree to manage I/O
chunking. Making the global octree redundant in this way raises the question
about whether the octree is really needed at all.

Currently yt makes a distinction between particle fields and mesh fields. All
SPH-smoothed fields (e.g. ``('gas', 'density')``) are smoothed onto the global
octree mesh. To make a concrete example, let's try loading an SPH zoom-in
simulation of a galaxy and ask for the ``('gas', 'density')`` field::

  import yt
  ds = yt.load('GadgetDiskGalaxy/snapshot_200.hdf5')

  ad = ds.all_data()
  density = ad['gas', 'density']

  print(density.shape)
  print(ds.particle_type_counts)

Running this script on the latest development version of yt at time of writing
(``abf5a8eff1b2``) produces the following output::

  (5661944,)
  {'PartType0': 4334546,
   'PartType1': 4786616,
   'PartType2': 2333848,
   'PartType3': 0,
   'PartType4': 450921,
   'PartType5': 1149}

On my laptop, this script also takes about 116 seconds to run, with 105 s spent
performing the SPH smoothing operation onto the global octree. Note also how the
number of leaf octs in the octree (5661944) does not match the number of SPH
particles (``PartType0``). This discrepancy is a common source of initial
confusion for users of SPH codes when they first try to use yt to analyze their
data.

We can ask ourselves whether it makes sense to always smooth data onto the
global octree. It makes intuitive sense for users of AMR codes if does
operations on a volume-filling mesh for AMR data, since the volume filling mesh
is the "real" data. However, for SPH data, the global octree mesh is not
representative of the "native" data. By making the return value of most yt
operations for SPH fields be defined on the octree mesh, yt is not being "true"
to the data and also makes it harder than it needs to be to access the particle
as such.

In this YTEP, **we propose changing the data object API for SPH data by ensuring
that all SPH smoothed fields return data defined at the locations of SPH
particles**. This means that rather than relying on smoothing data onto the
global octree, we will instead always return data defined at the particle
locations. This means that running the script included above would produce the
following output::

  (4334546,)
  {'PartType0': 4334546,
   'PartType1': 4786616,
   'PartType2': 2333848,
   'PartType3': 0,
   'PartType4': 450921,
   'PartType5': 1149}

And that the ``('gas', 'density')`` field would merely be an alias to the
``('PartType0', 'Density')`` field available on-disk. Since we no longer need to
smooth data onto the in-memory global octree, this substantially reduces the
memory needed to work with SPH data while simultaneously substantially improving
performance. Just as an example, in the version of the yt that implements this
YTEP, the script at the top of this section requires only 3.3 seconds to run.

The details of how this backward incompatible change to the user yt user
experience for SPH data will be implemented is detailed below. This includes all
design decisions that have been made in the prototype version of yt that
implements this YTEP. In addition, there are still several design decisions
about how to implement this YTEP that have not yet been decided on. For more
details about these issues, see the "Open Questions" section at the bottom of
this document.

SPH Fields
++++++++++

Data Selection for SPH Fields
+++++++++++++++++++++++++++++

Visualization of Slices and Projection
++++++++++++++++++++++++++++++++++++++

Deposition operations
+++++++++++++++++++++

Testing
+++++++

Open Questions
--------------

The Projection Data Object
**************************

Cut Regions
***********

Global Octree or Octree Subset Data Object
******************************************

Volume Rendering
****************

Community engagement
********************

yt 4.0?
*******
